{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üê∏ <font color=\"#40be46\">  JFrog Swampup 2024 MLOPs LAB - The Frog-Factor Authenticator </font> üê∏ \n",
        "\n",
        "Welcome to the Lab! \n",
        "\n",
        "\n",
        "At JFrog, we're always exploring new features and capabilities. Today, we're diving into the authentication market with a brilliant idea: the \"Frog-Factor\" authenticator. This unique tool will authenticate you by recognizing your face alongside the JFrog frog in the same photo!\n",
        "\n",
        "Your mission, if you choose to accept it, is to help us build the Frog-Factor authenticator.\n",
        "\n",
        "First up, we'll need an object-detection model to get us started. Fortunately, we don't have to start from scratch! There are existing models we can use. But will they work out of the box? And remember, we must develop this authenticator securely and reliably.\n",
        "\n",
        "As you work through the notebook, follow the cells in order:\n",
        "\n",
        "‚ú® - This icon means there's a task for you to complete before moving to the next cell.\n",
        "\n",
        "üëÄ - This icon provides information about what the next cell is doing.\n",
        "\n",
        "Let's get started! We're here to help you every step of the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "In the cell below, please replace /<USERID> with the userid you got for the labs. For example, if you are user3, it should look like this:\n",
        "\n",
        "userid = \"user3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "userid = <USERID>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYprO9zBKoY"
      },
      "source": [
        "# üê∏  <font color=\"#40be46\">  Lab1: Caching HuggingFace models in Artifactory </font> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQFU8IDvBKoY"
      },
      "source": [
        "## Configure the HuggingFace client to work through Artifactory\n",
        "\n",
        "We don't have to start from scratch!\n",
        "Luckily HuggingFace contains some great object detection models we can try out. \n",
        "Since we want to store the models in Artifactory, we'll need to configure the environment as follows.\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. Log in to the Artifactory training instance here: https://mlops17234464260.jfrog.io/\n",
        "2. From the projects dropdown list, select your project (\"mlops-userx\")\n",
        "3. Navigate to *Administration --> Repositories*.\n",
        "4. Click 'Create a Repository' and select *Remote*. Then, select *HuggingFaceML* .\n",
        "5. On the next page, you only need to provide the *Repository Key* (the repo name)  **The repository will be prefixed with your project name (\"mlops-userx\"). Please add the repository key \"hf-remote\"**  then click `Create Remote Repository`\n",
        "6. A dialog will open, suggesting to set up the HuggingFaceML client or Do It Later - click `Set Up HuggingFaceML client`.\n",
        "7. Enter your Artifactory password then click `Generate Token & Create Instructions`.\n",
        "8. Copy the *token* and paste it into the cell below, replacing the \\<IDENTITY_TOKEN> placeholder.\n",
        "\n",
        "üëÄ The next cell sets the environment variables such that the huggingface client which we'll use later does not fetch the model from the hugging_face hub, but rather from Artifactory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWHO4UF2BKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY-TOKEN>\n",
        "\n",
        "%env HF_ENDPOINT=https://mlops17234464260.jfrog.io/artifactory/api/huggingfaceml/mlops-$userid-hf-remote\n",
        "\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB36qj04BKoZ"
      },
      "source": [
        "## Download the required Python packages through Artifactory\n",
        "\n",
        "To use and test our model, we'll need some Python packages. Since we want to make sure we're using trusted and allowed packages, we'll get the packages from Artifactory.\n",
        "We'll configure the Python installations to go to fetch the packages from Artifactory.\n",
        "\n",
        "We've already configured a Pypi repository for you to use. Run the next cell to download the required dependencies. \n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. In Artifactory, go to our pypi repository: https://mlops17234464260.jfrog.io/ui/repos/tree/General/mlops-training-remote-pypi \n",
        "2. Click `Set Me Up` in the top bar, enter your Artifactory password then click `Generate Token & Create Instructions`. Then, click the `Install` tab.\n",
        "3. Copy the URL from the value of index-url (starting with \"https://...\")\n",
        "![](https://drive.google.com/uc?id=1Y6AiOkyb4P3EUg0YrLpHU-HSg7-2g4cu)\n",
        "4. Replace \\<ARTIFACTORY_PIP_REPOSITORY_URL> with the URL you copied.\n",
        "\n",
        "\n",
        "üö© **NOTE:** The following cell may take up to 3 minutes to complete. \n",
        " \n",
        "üö©  **The cell will show a warning at the end, requesting to restart the session. It is not required, please select `cancel` and continue as usual** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JevGqh2tBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up.\n",
        "!pip3 install qwak-sdk huggingface_hub ultralytics -i <ARTIFACTORY_PIP_REPOSITORY_URL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vu_iqU8BKoZ"
      },
      "source": [
        "## Python imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0tV1EBBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download, HfApi\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "import json\n",
        "import random\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import logging,shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1XYVieBKoZ"
      },
      "source": [
        "## Download the pre-trained model\n",
        "\n",
        "üëÄ We'll be using the [Yolov8](https://docs.ultralytics.com/) object detection pre-trained model. It's initially configured to only detect human faces.\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "Replace \\<YOUR_NAME> with your name. Don't forget to put it in quotes, e.g. \"Tom\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xk2YsfWBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the model and processor\n",
        "model_name = \"shirabendor/YOLOV8-oiv7\"\n",
        "weights = \"yolov8m-oiv7.pt\"\n",
        "config_file = \"./model/main/config.json\"\n",
        "name = \"<YOUR_NAME>\"\n",
        "\n",
        "try:\n",
        "    snapshot_download(repo_id=model_name, allow_patterns=[weights, \"mlops.zip\"], local_dir=\".\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(e)\n",
        "# unpack the other course materials and remove the default folder by colab\n",
        "!unzip mlops.zip\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiHbu-qBKoa"
      },
      "source": [
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "On the Artifactory training instance, navigate to your newly created *remote HuggingfaceML repository*.\n",
        "\n",
        "Or, Here is a direct link (replace x with your user number):  https://mlops17234464260.jfrog.io/artifactory/mlops-userx-hf-remote/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6fNTSzMBKoa"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "üëÄ The following cell defines some helper functions that will help us to test and develop the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isp8HlzUBKoa"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC04jdmPBKoa"
      },
      "source": [
        "# Inference function\n",
        "\n",
        "üëÄ The following cell defines the inference function. The \"predict\" function will get an image as input, and try to detect a human face in the image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jbrhBqNBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR) \n",
        "\n",
        "model = YOLO(weights)\n",
        "\n",
        "def infere(cheat=False, name=name):\n",
        "    \n",
        "    with open(config_file, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    classes        = config['classes']\n",
        "    target_classes = config['target_classes']\n",
        "    conf           = config['conf']\n",
        "    max_det        = config['max_det']\n",
        "\n",
        "    filename = 'photo.jpg'  # Default filename\n",
        "    if not cheat:\n",
        "        filename = take_photo()\n",
        "    else:\n",
        "      data = \"./model/main/img/tom.jpg\"\n",
        "      name = \"Tom Hanks\"\n",
        "      shutil.copy(data, filename)\n",
        "           \n",
        "\n",
        "    frame = cv2.imread(filename) \n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "    results = model.predict(source=frame, \n",
        "                            show=False, \n",
        "                            classes=classes, \n",
        "                            conf=conf,\n",
        "                            max_det=max_det)\n",
        "\n",
        "    # Extracting the names of detected classes\n",
        "    boxes = results[0].boxes\n",
        "\n",
        "     # Draw bounding boxes\n",
        "    for box in boxes:\n",
        "        label = model.names[int(box.cls)]\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Convert to integer coordinates\n",
        "  \n",
        "        if int(box.cls) in target_classes:\n",
        "          # Draw bounding box around detected object\n",
        "          cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)  # Colored box \n",
        "          cv2.putText(frame, \"Frog\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "        else:\n",
        "          cv2.rectangle(frame, (x1, y1), (x2, y2), (235, 222, 52), 3)  # Colored box \n",
        "          cv2.putText(frame, name, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  (235, 222, 52), 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go ahead and test the model! \n",
        "Please approve using the camera for the model to work.\n",
        "Once you have the video stream, click \"Capture\" (above the video block on the top left) to take a photo, and examine the model's output.\n",
        "Then, run the cell again and take a photo of yourself and the Jfrog frog. Did the model identify the frog?\n",
        "\n",
        "üö©**NOTE** \n",
        "The following cell may fail the first time because it runs in parallel to requesting access to the webcam. If it happens, just rerun the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQfWU2oiBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you can't or don't want to use your own picture, you can use the cheat sheet cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    infere(cheat=True)\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qlAqOYHBKoa"
      },
      "source": [
        "# üê∏  <font color=\"#40be46\">  Lab2: Securing models </font> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNyskbpyBKoa"
      },
      "source": [
        "## Block malicious models with Xray\n",
        "\n",
        "Our Yolo model did quite well identifying human faces, but we wanted it to also detect frogs.\n",
        "What can we do?\n",
        "Searching HuggingFace, there is a model that seems just right! \n",
        "\n",
        "But is it safe? Let's configure Xray for our HuggingFace repository and find out. \n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "Let's configure Xray to scan our HuggingFaceML remote repository.\n",
        "\n",
        "#### Complete the following steps:\n",
        "\n",
        "***Add the HuggingFaceML remote repository to the Xray index:***\n",
        "\n",
        "1. Navigate to *Administration --> Xray Settings --> Indexed Resources --> Repositories*.\n",
        "2. Search for your repository in the repositories list, and click on the ... menu. Click on `configure` then ensure that \"Scan All Artifacts\" is selected, then click on \"Apply\".\n",
        "3. Select the ... menu again and click \"Index Now\". No need to change anything in the dialog box that opens. \n",
        "4. Lastly, again on the ... menu, select \"Refresh Index Status\" and make sure the status shows 1/1(100%) i.e. our initla model was scanned successfully.\n",
        "\n",
        "For your convenience, we've already created a policy and a watch, so just adding your repository will be enough to kick off scanning. \n",
        "You can view the details here:\n",
        "\n",
        "https://mlops17234464260.jfrog.io/ui/admin/xray/policiesGovernance/policies/edit/block-malicious-models?projectKey=mlops-user2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAk8olXUBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    snapshot_download(repo_id=\"MustEr/best_model_for_identifying_frogs\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(\"\\n\\n\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8 Xray blocked model download due to violation of the 'Malicious Package' policy.\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the scanning results \n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "Navigate to your project's scans list (**remember to replace x with your student id**)\n",
        "\n",
        "https://mlops17234464260.jfrog.io/ui/scans-list/repositories/mlops-userx-hf-remote-cache/scan-descendants?projectKey=mlops-userx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPjVasqBKoa"
      },
      "source": [
        "# üê∏  <font color=\"#40be46\">  Lab3: Uploading an updated model to a local repository & deploying with Qwak </font> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGKsStrBKob"
      },
      "source": [
        "## Train the model to identify Frogs\n",
        "\n",
        "Unfortunately, the \"best_model_for_identifying_frogs\" was not safe and we cannot use it.\n",
        "But we still want to detect the frogs. Next, we will 'train' our original Yolo model to identify other objects, specifically frogs.\n",
        "\n",
        "üëÄ  Due to time constraints, our training function does not actually train on additional images. Instead, we'll just change the model configuration. Check the \"config.json\" file before and after the training to see the difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2xC_SQ6BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def train(object_to_detect):\n",
        "\n",
        "    if not object_to_detect in model.names.values():\n",
        "        print(f\"'{object_to_detect}' is not a valid YOLOv8 object. Hint: try Frog\")\n",
        "        return\n",
        "\n",
        "    reverse_dict = {name: idx for idx, name in model.names.items()}\n",
        "    class_id = reverse_dict.get(object_to_detect, None)\n",
        "\n",
        "    with open(config_file, 'r') as file:\n",
        "        config = json.load(file)\n",
        "\n",
        "    target_classes = config['target_classes']\n",
        "\n",
        "    # Add the new class number to the classes list if it's not already present\n",
        "    if class_id not in config['classes']:\n",
        "        config['classes'].append(class_id)\n",
        "        config['classes'].extend([cls for cls in target_classes if cls not in config['classes']])\n",
        "\n",
        "    config['max_det'] = 2\n",
        "\n",
        "    # Save the updated config back to the file\n",
        "    with open(config_file, 'w') as file:\n",
        "        json.dump(config, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvzi83nrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train(\"Frog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osJKMUJXBKob"
      },
      "source": [
        "## Run inference again\n",
        "\n",
        "Let's check to see if the training did the trick!\n",
        "Please take the JFrog frog and take a photo of the two of you together üòä üê∏ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ZkKdhrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, a cheat sheet cell is available if you like in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    infere(cheat=True)\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j54q37oHBKob"
      },
      "source": [
        "## Upload to HF local\n",
        "\n",
        "Now that we have a new, trained model, we need to upload it to the Artifactory HugginigFaceML local repository in order to share it with other teams and promote it towards Production.\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font>‚ú®\n",
        "\n",
        "Please perform the following steps:\n",
        "1. Navigate to *Administration --> Repositories*.\n",
        "2. From the projects dropdown list, select your project (\"mlops-userx\").  \n",
        "3. Click 'Create a Repository' and select **Local**. Select HuggingFaceML.\n",
        "4. On the next page, only provide the Repository Key (the repo name). The repository will be prefixed with your project name (\"mlops-userx\"). Please add the repository key **\"hf-local\"** then click Create Local Repository.\n",
        "5. A dialog will open, suggesting to set up the HuggingFaceML client or Do It Later - click `Set Up HuggingFaceML client`.\n",
        "6. Enter your Artifactory password then click `Generate Token & Create Instructions`.\n",
        "7. Copy the *token* and paste it into the cell below, replacing the \\<IDENTITY_TOKEN> placeholder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A580wraBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY-TOKEN>\n",
        "\n",
        "# Replace the <PATH> placeholder with the path to your ML Model Management repository in Artifactory, found in the JFrog Platform SetMeUp.\n",
        "%env HF_ENDPOINT=https://mlops17234464260.jfrog.io/artifactory/api/huggingfaceml/mlops-$userid-hf-local\n",
        "\n",
        "%env HF_HUB_DOWNLOAD_TIMEOUT=86400\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiWAoQ2BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "# Initialize API with the custom endpoint\n",
        "api = HfApi(endpoint=os.getenv(\"HF_ENDPOINT\"))\n",
        "\n",
        "# Upload folder to the specified repository\n",
        "api.upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=\"frog-factor1\",   # Replace with a name for your model\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7A30IlOBKob"
      },
      "source": [
        "### Check the results in Artifactory\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "\n",
        "1. On the Artifactory training instance, navigate to *Artifactory --> Artifacts* tab.\n",
        "2. Find your newly created *local HuggingFaceML repository*.\n",
        "3. Expand the repository and verify the YOLOV8 model is cached inside the repository, including the updated configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytvkoKZnBKob"
      },
      "source": [
        "## Deploy with Qwak \n",
        "\n",
        "Now that we have a good model version, let's deploy it to a production endpoint with Qwak and monitor its performance.\n",
        "Qwak is a fully managed end-to-end platform that contains the infrastructure AI practitioners need to build, deploy, manage and monitor GenAI, LLMs and classic ML in production.\n",
        "\n",
        "We've already installed the Qwak SDK.\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. Create a personal API key in the Qwak platform:\n",
        "    - Go to [Qwak Platform](https://app.qwak.ai/), then login with your email and the password that was given to you for the training.\n",
        "    - On the left hand side menu, Navigate to *Settings --> Personal API Keys*.\n",
        "    - Click `Generate API Key`.\n",
        "    - Copy the API key generated and replace the below <QWAK_PERSONAL_API_KEY> placeholder with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXA_3Xd7BKob"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Replace <QWAK_PERSONAL_API_KEY> with your Qwak personal key from the qwak platform.\n",
        "!qwak configure --api-key \"<QWAK_PERSONAL_API_KEY>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFCWoxaaBKob"
      },
      "source": [
        "### Build the Qwak model\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "In order to build and deploy the model through the Qwak platform, run the following commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w9t_IXnBKoc"
      },
      "outputs": [],
      "source": [
        "model_id = f\"frog_factor_authenticator_{userid}\"\n",
        "!qwak models create \"{model_id}\" --project $userid       \n",
        "!qwak models build --model-id $model_id ./model --base-image 'public.ecr.aws/w8k8y6b6/qwak-base:0.0.14-gpu-opencv' --gpu-compatible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSB5ey-WBKoc"
      },
      "source": [
        "### Check your model build status (can take up to 30 minutes)\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. In the [Qwak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click your model.\n",
        "3. Under the *Builds* tab, identify your build and check the status."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## While we're waiting, let's do some detections with a model which is already on the Qwak platform.\n",
        "\n",
        "We've already deployed the updated model into the Qwak platform in order to be able to test it without waiting for the build to complete. \n",
        "Let's check our deployment and see if we're able to authenticate with the Frog-Factor Authenticator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some More Helper Functions\n",
        "\n",
        "üëÄ The following cell defines some helper functions that adds some overlays based on the response we get from the Qwak model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qwak_inference import RealTimeClient as QwakClient\n",
        "from PIL import Image\n",
        "import numpy as np \n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def overlay_image_alpha(img, img_overlay, position=\"top\"):\n",
        "    \"\"\"Overlay img_overlay on top of img at the specified position.\"\"\"\n",
        "\n",
        "    if position == \"top\":\n",
        "        # Image ranges for top position\n",
        "        y1, y2 = 0, min(img.shape[0], img_overlay.shape[0])\n",
        "        x1, x2 = 0, min(img.shape[1], img_overlay.shape[1])\n",
        "    elif position == \"bottom\":\n",
        "        # Image ranges for middle-bottom position\n",
        "        y1 = max(0, img.shape[0] - img_overlay.shape[0])\n",
        "        y2 = img.shape[0]\n",
        "        x1 = max(0, (img.shape[1] - img_overlay.shape[1]) // 2)\n",
        "        x2 = min(img.shape[1], x1 + img_overlay.shape[1])\n",
        "    else:\n",
        "        raise ValueError(\"Position must be either 'top' or 'bottom'.\")\n",
        "\n",
        "    # Overlay ranges\n",
        "    y1o, y2o = 0, min(img_overlay.shape[0], y2 - y1)\n",
        "    x1o, x2o = 0, min(img_overlay.shape[1], x2 - x1)\n",
        "\n",
        "    # Exit if nothing to do\n",
        "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
        "        return img\n",
        "\n",
        "    # Blend overlay within the determined ranges\n",
        "    img_crop = img[y1:y2, x1:x2]\n",
        "    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n",
        "\n",
        "    # Split the alpha channel and the color channels\n",
        "    if img_overlay_crop.shape[2] == 4:  # Ensure the overlay has an alpha channel\n",
        "        img_overlay_color = img_overlay_crop[:, :, :3]\n",
        "        alpha_mask = img_overlay_crop[:, :, 3] / 255.0\n",
        "\n",
        "        # Reverse the color channels for BGR format\n",
        "        img_overlay_color = img_overlay_color[:, :, ::-1]\n",
        "\n",
        "        alpha_inv = 1.0 - alpha_mask\n",
        "\n",
        "        for c in range(0, 3):\n",
        "            img_crop[:, :, c] = (alpha_mask * img_overlay_color[:, :, c] +\n",
        "                                 alpha_inv * img_crop[:, :, c])\n",
        "    else:\n",
        "        img_crop[:, :, :] = img_overlay_crop\n",
        "\n",
        "    return img\n",
        "\n",
        "def analze_results(results):\n",
        "\n",
        "    with open(config_file, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    target_classes = config['target_classes']\n",
        "\n",
        "    #  Load the success image\n",
        "    success_img = Image.open('model/main/img/success.png')\n",
        "    success_rgba = np.array(success_img.convert('RGBA'))\n",
        "\n",
        "    powered_by_img = Image.open('model/main/img/powered_by.png')\n",
        "    powered_by_rgba_rgba = np.array(success_img.convert('RGBA')) \n",
        "\n",
        "    # Main processing loop\n",
        "    target_detected = False\n",
        "\n",
        "    frame = cv2.imread('photo.jpg')\n",
        "\n",
        "    for result in results:\n",
        "        box = result[\"box\"]\n",
        "        x1, y1, x2, y2 = map(int, box.values())  # Convert to integer coordinates\n",
        "\n",
        "        if int(result[\"class\"]) in target_classes:\n",
        "            # Draw bounding box around detected object\n",
        "            target_detected = True\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)  # Colored box \n",
        "            cv2.putText(frame, \"Frog\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "        else:\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (235, 222, 52), 3)  # Colored box \n",
        "            cv2.putText(frame, name, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (235, 222, 52), 2)\n",
        "\n",
        "    if target_detected:\n",
        "    # Overlay the success image at the top of the frame\n",
        "        blended =overlay_image_alpha(frame, success_rgba)\n",
        "        cv2_imshow(blended)\n",
        "    else:\n",
        "        # Display \"Authentication Failed\" text\n",
        "        cv2.putText(frame, \"Authentication Failed\", (frame.shape[1] // 2 - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        \n",
        "    cv2_imshow(frame)\n",
        "\n",
        "\n",
        "def send_image_to_qwak(image):\n",
        "    img = Image.open(image)\n",
        "    image_rgb = img.convert('RGB')\n",
        "    img_ndarray = np.array(image_rgb)\n",
        "    img_list = img_ndarray.tolist()\n",
        "\n",
        "    client = QwakClient(model_id=\"yolo_test_2\")\n",
        "\n",
        "    response = client.predict(img_list)\n",
        "\n",
        "    results = response[0][\"results\"]\n",
        "\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üëÄ  We'll take another photo for the inference. Do not forget the frog! The detection boxes will not appear, we will perform the detection later on the Qwak platform.\n",
        "If you wish to use Tom Hanks' photo instead, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imagefile = take_photo()\n",
        "frame = cv2.imread('photo.jpg') \n",
        "cv2_imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = send_image_to_qwak('photo.jpg') # change to results = send_image_to_qwak('tom.jpg') if you wish to use Tom's picture\n",
        "analze_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's monitor the model performane\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. In the [Qwak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select \"yolo\" project and click \"yolo-test-2\"\n",
        "3. Monitor the status under the \"overview\" tab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üê∏  <font color=\"#40be46\">  Bonus Lab: Deploy your model to production endpoint in Qwak  </font> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awRTEZCdBKoc"
      },
      "source": [
        "### Deploy your model\n",
        "\n",
        "‚ú® <font color=\"#f8c76b\"> TASK </font> ‚ú®\n",
        "\n",
        "1. In the [Qwak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click your model.\n",
        "3. Under the *Builds* tab, identify your build and ensure it finished building successfully, Then click `Deploy`. \n",
        "4. Select `Realtime`.\n",
        "5. On the next screen, click `Deploy Model`. No need to change anything.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf-demo-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
