{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üê∏ <span style='color :#40be46' > JFrog Swampup 2024 MLOPs LAB - The Frog-Factor Authenticator </span> üê∏ \n",
        "\n",
        "Welcome to the Lab! \n",
        "\n",
        "\n",
        "At JFrog, we're always exploring new features and capabilities. Today, we're diving into the authentication market with a brilliant idea: the \"Frog-Factor\" authenticator. This unique tool will authenticate you by recognizing your face alongside the JFrog frog in the same photo!\n",
        "\n",
        "Your mission, if you choose to accept it, is to help us build the Frog-Factor authenticator.\n",
        "\n",
        "First up, we'll need an object-detection model to get us started. Fortunately, we don't have to start from scratch! There are existing models we can use. But will they work out of the box? And remember, we must develop this authenticator securely and reliably.\n",
        "\n",
        "As you work through the notebook, follow the cells in order:\n",
        "\n",
        "‚ú® - This icon means there's a task for you to complete before moving to the next cell.\n",
        "\n",
        "üëÄ - This icon provides information about what the next cell is doing.\n",
        "\n",
        "Let's get started! We're here to help you every step of the way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYprO9zBKoY"
      },
      "source": [
        "# üê∏  <span style='color :#40be46' > Lab1: Caching HuggingFace models in Artifactory </span> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQFU8IDvBKoY"
      },
      "source": [
        "## Configure the HuggingFace client to work through Artifactory\n",
        "\n",
        "We don't have to start from scratch!\n",
        "Luckily HuggingFace contains some great object detection models we can try out. \n",
        "Since we want to store the models in Artifactory, we'll need to configure the environment as follows.\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. Log in to the Artifactory training instance here: https://jftrain17224572670.jfrog.io/\n",
        "2. From the projects dropdown list, select your project (\"mlops-userx\")\n",
        "3. Navigate to *Administration --> Repositories*.\n",
        "4. Click 'Create a Repository' and select *Remote*. Then, select *HuggingFaceML* .\n",
        "5. On the next page, you only need to provide the *Repository Key* (the repo name)  **The repository will be prefixed with your project name (\"mlops-userx\"). Please add the repository key \"hf-remote\"**  then click `Create Remote Repository`\n",
        "6. A dialog will open, suggesting to set up the HuggingFaceML client or Do It Later - click `Set Up HuggingFaceML client`.\n",
        "7. Enter your Artifactory password then click `Generate Token & Create Instructions`.\n",
        "8. Copy the *token* and paste it into the cell below, replacing the \\<IDENTITY_TOKEN> placeholder.\n",
        "9. Replace x (in *userx-hf-remote* inside the HF_ENDPOINT env var) with your lab studentId number.\n",
        "\n",
        "üëÄ The next cell sets the environment variables such that the huggingface client which we'll use later does not fetch the model from the hugging_face hub, but rather from Artifactory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWHO4UF2BKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY-TOKEN>\n",
        "\n",
        "# Replace x (in userx-hf-remote) with your userid number\n",
        "%env HF_ENDPOINT=https://jftrain17224572670.jfrog.io/artifactory/api/huggingfaceml/mlops-userx-hf-remote\n",
        "\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB36qj04BKoZ"
      },
      "source": [
        "## Download the required Python packages through Artifactory\n",
        "\n",
        "To use and test our model, we'll need some Python packages. Since we want to make sure we're using trusted and allowed packages, we'll get the packages from Artifactory.\n",
        "We'll configure the Python installations to go to fetch the packages from Artifactory.\n",
        "\n",
        "We've already configured a Pypi repository for you to use. Run the next cell to download the required dependencies. \n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. In Artifactory, go to our pypi repository: https://jftrain17224572670.jfrog.io/ui/repos/tree/General/mlops-training-remote-pypi \n",
        "2. Click `Set Me Up` in the top bar, enter your Artifactory password then click `Generate Token & Create Instructions`. Then, click the `Install` tab.\n",
        "3. Copy the URL from the value of index-url (starting with \"https://...\")\n",
        "4. Replace \\<ARTIFACTORY_PIP_REPOSITORY_URL> with the URL you copied.\n",
        "\n",
        "\n",
        "*NOTE:*\n",
        "The following cell may take up to 3 minutes to complete. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JevGqh2tBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up.\n",
        "!pip3 install huggingface_hub ultralytics -i <ARTIFACTORY_PIP_REPOSITORY_URL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vu_iqU8BKoZ"
      },
      "source": [
        "## Python imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0tV1EBBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download, HfApi\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "import json\n",
        "import random\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import logging,shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1XYVieBKoZ"
      },
      "source": [
        "## Download the pre-trained model\n",
        "\n",
        "üëÄ We'll be using the Yolov8 object detection pre-trained model. It's initially configured to only detect human faces.\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Replace \\<YOUR_NAME> with your name. Don't forget to put it in quotes, e.g. \"Tom\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xk2YsfWBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the model and processor\n",
        "model_name = \"shirabendor/YOLOV8-oiv7\"\n",
        "weights = \"yolov8m-oiv7.pt\"\n",
        "config_file = \"./model/main/config.json\"\n",
        "name = \"<YOUR_NAME>\"\n",
        "\n",
        "try:\n",
        "    snapshot_download(repo_id=model_name, allow_patterns=[weights, \"mlops.zip\"], local_dir=\".\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(\"\\n\\n\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8 Xray blocked model download due to violation of the `Block Malicious Packages` policy.\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8\\n\\n\")\n",
        "\n",
        "# unpack the other course materials and remove the default folder by colab\n",
        "!unzip mlops.zip\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiHbu-qBKoa"
      },
      "source": [
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "On the Artifactory training instance, navigate to your newly created *remote HuggingfaceML repository*.\n",
        "\n",
        "Or, Here is a direct link (replace x with your user number):  https://jftrain17224572670.jfrog.io/artifactory/mlops-userx-hf-remote/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6fNTSzMBKoa"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "üëÄ The following cell defines some helper functions that will help us to test and develop the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isp8HlzUBKoa"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC04jdmPBKoa"
      },
      "source": [
        "# Inference function\n",
        "\n",
        "üëÄ The following cell defines the inference function. The \"predict\" function will get an image as input, and try to detect a human face in the image. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jbrhBqNBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR) \n",
        "\n",
        "model = YOLO(weights)\n",
        "\n",
        "def infere(cheat=False, name=name):\n",
        "    \n",
        "    with open(config_file, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    classes        = config['classes']\n",
        "    target_classes = config['target_classes']\n",
        "    conf           = config['conf']\n",
        "    max_det        = config['max_det']\n",
        "\n",
        "    filename = 'photo.jpg'  # Default filename\n",
        "    if not cheat:\n",
        "        filename = take_photo()\n",
        "    else:\n",
        "      data = \"./model/main/img/tom.jpg\"\n",
        "      name = \"Tom Hanks\"\n",
        "      shutil.copy(data, filename)\n",
        "           \n",
        "\n",
        "    frame = cv2.imread(filename) \n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "    results = model.predict(source=frame, \n",
        "                            show=False, \n",
        "                            classes=classes, \n",
        "                            conf=conf,\n",
        "                            max_det=max_det)\n",
        "\n",
        "    # Extracting the names of detected classes\n",
        "    boxes = results[0].boxes\n",
        "\n",
        "     # Draw bounding boxes\n",
        "    for box in boxes:\n",
        "        label = model.names[int(box.cls)]\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Convert to integer coordinates\n",
        "  \n",
        "        if int(box.cls) in target_classes:\n",
        "          # Draw bounding box around detected object\n",
        "          cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)  # Colored box \n",
        "          cv2.putText(frame, \"Frog\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
        "        else:\n",
        "          cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)  # Colored box \n",
        "          cv2.putText(frame, name, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,  (255, 0, 0), 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go ahead and test the model! \n",
        "Please approve using the camera for the model to work.\n",
        "Once you have the video stream, click \"Capture\" (above the video block on the top left) to take a photo, and examine the model's output.\n",
        "Then, run the cell again and take a photo of yourself and the Jfrog frog. Did the model identify the frog?\n",
        "\n",
        "*NOTE:*\n",
        "The following cell may fail the first time because it runs in parallel to requesting access to the webcam. If it happens, just rerun the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQfWU2oiBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case you can't or don't want to use your own picture, you can use the cheat sheet cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    infere(cheat=True)\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qlAqOYHBKoa"
      },
      "source": [
        "# üê∏  <span style='color :#40be46' > Lab2: Securing models </span> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNyskbpyBKoa"
      },
      "source": [
        "## Block malicious models with Xray\n",
        "\n",
        "Our Yolo model did quite well identifying human faces, but we wanted it to also detect frogs.\n",
        "What can we do?\n",
        "Searching HuggingFace, there is a model that seems just right! \n",
        "\n",
        "But is it safe? Let's configure Xray for our HuggingFace repository and find out. \n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Let's configure Xray to scan our HuggingFaceML remote repository.\n",
        "\n",
        "#### Complete the following steps:\n",
        "\n",
        "***Add the HuggingFaceML remote repository to the Xray index:***\n",
        "\n",
        "1. Navigate to *Administration --> Xray Settings --> Indexed Resources --> Repositories*.\n",
        "2. Search for your repository in the repositories list, and click on the ... menu. Click on `configure` then ensure that \"Scan All Artifacts\" is selected, then click on \"Apply\".\n",
        "3. Select the ... menu again and click \"Index Now\". No need to change anything in the dialog box that opens. \n",
        "4. Lastly, again on the ... menu, refresh the status to see the results.\n",
        "\n",
        "For your convenience, we've already created a policy and a watch, so just adding your repository will be enough to kick off scanning. \n",
        "You can examine them in the following links:\n",
        "\n",
        "1. Policy: https://jftrain17224572670.jfrog.io/ui/admin/xray/policiesGovernance/policies/edit/block-malicious-models\n",
        "2. Watch: https://jftrain17224572670.jfrog.io/ui/admin/xray/policiesGovernance/watches/edit/bloack-malicious-model-watch?activeTab=violations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAk8olXUBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    snapshot_download(repo_id=\"MustEr/best_model_for_identifying_frogs\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(\"\\n\\n\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8 Xray blocked model download due to violation of the 'Malicious Package' policy.\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the scanning results \n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Navigate to your project's scans list (**remembe to replace x with your student id**)\n",
        "\n",
        "https://jftrain17224572670.jfrog.io/ui/scans-list/repositories?projectKey=mlops-userx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPjVasqBKoa"
      },
      "source": [
        "# üê∏  <span style='color :#40be46' > Lab3: Uploading an updated model to a local repository and deploying with Qwak </span> üê∏ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGKsStrBKob"
      },
      "source": [
        "## Train the model to identify Frogs\n",
        "\n",
        "Unfortunately, the \"best_model_for_identifying_frogs\" was not safe and we cannot use it.\n",
        "But we still want to detect the frogs. Next, we will 'train' our original Yolo model to identify other objects, specifically frogs.\n",
        "\n",
        "üëÄ  Due to time constraints, our training function does not actually train on additional images. Instead, we'll just change the model configuration. Check the \"config.json\" file before and after the training to see the difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2xC_SQ6BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def train(object_to_detect):\n",
        "\n",
        "    if not object_to_detect in model.names.values():\n",
        "        print(f\"'{object_to_detect}' is not a valid YOLOv8 object. Hint: try Frog\")\n",
        "        return\n",
        "\n",
        "    reverse_dict = {name: idx for idx, name in model.names.items()}\n",
        "    class_id = reverse_dict.get(object_to_detect, None)\n",
        "\n",
        "    with open(config_file, 'r') as file:\n",
        "        config = json.load(file)\n",
        "\n",
        "    target_classes = config['target_classes']\n",
        "\n",
        "    # Add the new class number to the classes list if it's not already present\n",
        "    if class_id not in config['classes']:\n",
        "        config['classes'].append(class_id)\n",
        "        config['classes'].extend([cls for cls in target_classes if cls not in config['classes']])\n",
        "\n",
        "    config['max_det'] = 2\n",
        "\n",
        "    # Save the updated config back to the file\n",
        "    with open(config_file, 'w') as file:\n",
        "        json.dump(config, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvzi83nrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train(\"Frog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osJKMUJXBKob"
      },
      "source": [
        "## Run inference again\n",
        "\n",
        "Let's check to see if the training did the trick!\n",
        "Please take the JFrog frog and take a photo of the two of you together üòä üê∏ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ZkKdhrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, a cheat sheet cell is available if you like in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    infere(cheat=True)\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j54q37oHBKob"
      },
      "source": [
        "## Upload to HF local\n",
        "\n",
        "Now that we have a new, trained model, we need to upload it to the Artifactory HugginigFaceML local repository in order to share it with other teams and promote it towards Production.\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Please perform the following steps:\n",
        "1. Navigate to *Administration --> Repositories*.\n",
        "2. From the projects dropdown list, select your project (\"mlops-userx\").  \n",
        "3. Click 'Create a Repository' and select **Local**. Select HuggingFaceML.\n",
        "4. On the next page, only provide the Repository Key (the repo name). The repository will be prefixed with your project name (\"mlops-userx\"). Please add the repository key **\"hf-local\"** then click Create Local Repository.\n",
        "5. A dialog will open, suggesting to set up the HuggingFaceML client or Do It Later - click `Set Up HuggingFaceML client`.\n",
        "6. Enter your Artifactory password then click `Generate Token & Create Instructions`.\n",
        "7. Copy the *token* and paste it into the cell below, replacing the \\<IDENTITY_TOKEN> placeholder.\n",
        "8. Replace x (in **userx-hf-local** inside the HF_ENDPOINT env var) with your lab studentId number.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A580wraBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY-TOKEN>\n",
        "\n",
        "# Replace the <PATH> placeholder with the path to your ML Model Management repository in Artifactory, found in the JFrog Platform SetMeUp.\n",
        "%env HF_ENDPOINT=<PATH>\n",
        "\n",
        "%env HF_HUB_DOWNLOAD_TIMEOUT=86400\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiWAoQ2BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "# Initialize API with the custom endpoint\n",
        "api = HfApi(endpoint=os.getenv(\"HF_ENDPOINT\"))\n",
        "\n",
        "# Upload folder to the specified repository\n",
        "api.upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=\"frog-factor1\",   # Replace with a name for your model\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7A30IlOBKob"
      },
      "source": [
        "### Check the results in Artifactory\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "\n",
        "1. On the Artifactory training instance, navigate to *Artifactory --> Artifacts* tab.\n",
        "2. Find your newly created *local HuggingFaceML repository*.\n",
        "3. Expand the repository and verify the YOLOV8 model is cached inside the repository, including the updated configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytvkoKZnBKob"
      },
      "source": [
        "## Deploy with Qwak\n",
        "\n",
        "Now that we have a good model version, let's deploy it to a production endpoint with Qwak and monitor its performance.\n",
        "Qwak is a fully managed end-to-end platform that contains the infrastructure AI practitioners need to build, deploy, manage and monitor GenAI, LLMs and classic ML in production.\n",
        "\n",
        "We'll start by installing the Qwak SDK.\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up (you can take it from the cell in the first lab).\n",
        "2. Create a personal API key in the Qwak platform:\n",
        "    - Go to [Quak Platform](https://app.qwak.ai/), then login with your email and the password that was given to you for the training.\n",
        "    - On the left hand side menu, Navigate to *Settings --> Personal API Keys*.\n",
        "    - Click `Generate API Key`.\n",
        "    - Copy the API key generated and replace the below <QWAK_PERSONAL_API_KEY> placeholder with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXA_3Xd7BKob"
      },
      "outputs": [],
      "source": [
        "# Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up.\n",
        "!pip3 install qwak-sdk --use-deprecated=legacy-resolver -i <ARTIFACTORY_PIP_REPOSITORY_URL>\n",
        "# Replace <QWAK_PERSONAL_API_KEY> with your Qwak personal key from the qwak platform.\n",
        "!qwak configure --api-key \"<QWAK_PERSONAL_API_KEY>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFCWoxaaBKob"
      },
      "source": [
        "### Build the Qwak model\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "In order to build and deploy the model through the Qwak platform, run the following commands.\n",
        "1. In the cell below, replace \\<userx> with your user id (e.g. \"user3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w9t_IXnBKoc"
      },
      "outputs": [],
      "source": [
        "!qwak models create \"frog_factor_authenticator\" --project <userx>         \n",
        "!qwak models build --model-id \"frog_factor_authenticator\" ./model --base-image 'public.ecr.aws/w8k8y6b6/qwak-base:0.0.14-gpu-opencv' --gpu-compatible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSB5ey-WBKoc"
      },
      "source": [
        "### Check your model build status (can take up to 30 minutes)\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. In the [Quak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click your model.\n",
        "3. Under the *Builds* tab, identify your build and check the status."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## While we're waiting...\n",
        "\n",
        "We've already deployed the model in order to be able to test it without waiting for the build to complete. Let's check our deployment and see if we're able to authenticate with the Frog-Factor Authenticator.\n",
        "\n",
        "üëÄ  We'll take another photo for the inference. Do not forget the frog! The detection boxes will not appear, we will perform the detection later on the Qwak platform.\n",
        "If you wish to use Tom Hank's photo instead, don't run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imagefile = take_photo()\n",
        "frame = cv2.imread('photo.jpg') \n",
        "cv2_imshow(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qwak_inference import RealTimeClient\n",
        "from PIL import Image\n",
        "import numpy as np \n",
        "import json\n",
        "import cv2\n",
        "\n",
        "def overlay_image_alpha(img, img_overlay):\n",
        "    \"\"\"Overlay img_overlay on top of img at the position specified by pos.\"\"\"\n",
        "\n",
        "    # Image ranges\n",
        "    y1, y2 = 0, min(img.shape[0], img_overlay.shape[0])\n",
        "    x1, x2 = 0, min(img.shape[1], img_overlay.shape[1])\n",
        "\n",
        "    # Overlay ranges\n",
        "    y1o, y2o = 0, min(img_overlay.shape[0], img.shape[0])\n",
        "    x1o, x2o = 0, min(img_overlay.shape[1], img.shape[1])\n",
        "\n",
        "    # Exit if nothing to do\n",
        "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
        "        return img\n",
        "\n",
        "    # Blend overlay within the determined ranges\n",
        "    img_crop = img[y1:y2, x1:x2]\n",
        "    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n",
        "\n",
        "    # Split the alpha channel and the color channels\n",
        "    if img_overlay_crop.shape[2] == 4:  # Ensure the overlay has an alpha channel\n",
        "        img_overlay_color = img_overlay_crop[:, :, :3]\n",
        "        alpha_mask = img_overlay_crop[:, :, 3] / 255.0\n",
        "\n",
        "        alpha_inv = 1.0 - alpha_mask\n",
        "\n",
        "        for c in range(0, 3):\n",
        "            img_crop[:, :, c] = (alpha_mask * img_overlay_color[:, :, c] +\n",
        "                                 alpha_inv * img_crop[:, :, c])\n",
        "    else:\n",
        "        img_crop[:, :, :] = img_overlay_crop\n",
        "\n",
        "    return img\n",
        "\n",
        "with open(config_file, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "target_classes = config['target_classes']\n",
        "\n",
        "# Load the success image\n",
        "success_img = Image.open('model/main/img/success.png')\n",
        "\n",
        "success_rgba = success_img.convert('RGBA')\n",
        "success_rgba = np.array(success_rgba)\n",
        "\n",
        "# Main processing loop\n",
        "target_detected = False\n",
        "\n",
        "img = Image.open('photo.jpg')\n",
        "image_rgb = img.convert('RGB')\n",
        "img_ndarray = np.array(image_rgb)\n",
        "img_list = img_ndarray.tolist()\n",
        "img_json = json.dumps(img_list)\n",
        "\n",
        "frame = cv2.imread('photo.jpg')\n",
        "\n",
        "client = RealTimeClient(model_id=\"yolo_test_2\")\n",
        "\n",
        "response = client.predict(img_list)\n",
        "\n",
        "results = response[0][\"results\"]\n",
        "\n",
        "for result in results:\n",
        "    label = result[\"name\"]\n",
        "    box = result[\"box\"]\n",
        "    x1, y1, x2, y2 = map(int, box.values())  # Convert to integer coordinates\n",
        "\n",
        "    if int(result[\"class\"]) in target_classes:\n",
        "        # Draw bounding box around detected object\n",
        "        target_detected = True\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 3)  # Colored box \n",
        "        cv2.putText(frame, \"Frog\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "    else:\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (208, 118, 4), 3)  # Colored box \n",
        "        cv2.putText(frame, label, (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (208, 118, 4), 2)\n",
        "\n",
        "if target_detected:\n",
        "    # Overlay the success image at the top of the frame\n",
        "    blended =overlay_image_alpha(frame, success_rgba)\n",
        "    cv2_imshow(blended)\n",
        "else:\n",
        "    # Display \"Authentication Failed\" text\n",
        "    cv2.putText(frame, \"Authentication Failed\", (frame.shape[1] // 2 - 150, 50), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's monitor the model performane\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. In the [Quak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click your model.\n",
        "3. Monitor the status under the \"overview\" tab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awRTEZCdBKoc"
      },
      "source": [
        "### Deploy your model\n",
        "\n",
        "‚ú® <span style='color : #fae253' > TASK </span> ‚ú®\n",
        "\n",
        "1. In the [Quak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click your model.\n",
        "3. Under the *Builds* tab, identify your build and click `Deploy`. This is only possible after the model finished building.\n",
        "4. Select `Realtime`.\n",
        "5. On the next screen, click `Deploy Model`. No need to change anything.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf-demo-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
